import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import joblib

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix
from sklearn.feature_selection import SelectFromModel

# Set random seed for reproducibility
RANDOM_SEED = 42

# ------------------ Step 1: Load Data ------------------
def load_data(file_path):
    """Loads the dataset from a CSV file."""
    df = pd.read_csv(file_path)
    return df

# ------------------ Step 2: Data Preprocessing ------------------
def preprocess_data(df, selected_num_features, cat_features):
    """Preprocesses the data: handling missing values, encoding, and scaling."""
    
    # Drop transaction ID (not useful for modeling)
    df.drop(columns=["transactionid"], inplace=True, errors='ignore')

    # Handle missing values
    df.fillna(df.median(numeric_only=True), inplace=True)  # Fill missing numerics
    df.fillna("Unknown", inplace=True)  # Fill missing categoricals

    # Encoding categorical features & scaling numerical features
    preprocessor = ColumnTransformer(
        transformers=[
            ("num", StandardScaler(), selected_num_features),
            ("cat", OneHotEncoder(handle_unknown="ignore", sparse_output=False), cat_features),
        ]
    )

    return df, preprocessor

# ------------------ Step 3: Remove Multicollinearity ------------------
def remove_multicollinearity(df, num_features, threshold=0.8):
    """Removes features with high correlation to avoid multicollinearity."""
    corr_matrix = df[num_features].corr().abs()
    upper_triangle = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))

    # Find highly correlated features
    to_drop = [column for column in upper_triangle.columns if any(upper_triangle[column] > threshold)]
    
    selected_features = [col for col in num_features if col not in to_drop]
    
    return selected_features

# ------------------ Step 4: Feature Selection (After Preprocessing) ------------------
def select_important_features(X_train_transformed, y_train):
    """Selects important features using logistic regression with L1 regularization."""
    
    # Train logistic regression on transformed data
    model = LogisticRegression(penalty="l1", solver="liblinear", class_weight="balanced", max_iter=200)
    model.fit(X_train_transformed, y_train)
    
    # Select important features
    selector = SelectFromModel(model, prefit=True)
    selected_indices = selector.get_support(indices=True)

    return selected_indices  # Returning indices instead of names (since data is transformed)

# ------------------ Step 5: Train Logistic Regression Model ------------------
def train_model(X_train, y_train):
    """Trains a logistic regression model with hyperparameter tuning."""
    
    model = LogisticRegression(class_weight="balanced", solver="liblinear", max_iter=200)

    # Hyperparameter tuning using GridSearchCV
    param_grid = {"C": [0.01, 0.1, 1, 10]}
    
    grid_search = GridSearchCV(model, param_grid, scoring="roc_auc", cv=5, n_jobs=-1)
    grid_search.fit(X_train, y_train)

    return grid_search.best_estimator_

# ------------------ Step 6: Evaluate Model ------------------
def evaluate_model(model, X_test, y_test):
    """Evaluates model performance using ROC-AUC, confusion matrix, and classification report."""
    y_pred = model.predict(X_test)
    y_prob = model.predict_proba(X_test)[:, 1]

    print("ROC-AUC Score:", roc_auc_score(y_test, y_prob))
    print("\nClassification Report:\n", classification_report(y_test, y_pred))
    
    # Confusion Matrix
    cm = confusion_matrix(y_test, y_pred)
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
    plt.xlabel("Predicted")
    plt.ylabel("Actual")
    plt.title("Confusion Matrix")
    plt.show()

# ------------------ Step 7: Save Model ------------------
def save_model(model, filename="fraud_detection_model.pkl"):
    """Saves the trained model for production deployment."""
    joblib.dump(model, filename)
    print(f"Model saved as {filename}")

# ------------------ Main Execution ------------------
if __name__ == "__main__":
    # Load Data
    df = load_data("fraud_data.csv")  # Update with actual file path

    # Define feature types
    target = "FroInd"
    num_features = df.select_dtypes(include=["int64", "float64"]).columns.tolist()
    cat_features = df.select_dtypes(include=["object"]).columns.tolist()
    num_features.remove(target)  # Remove target from numeric features

    # Remove highly correlated numeric features
    selected_num_features = remove_multicollinearity(df, num_features)

    # Preprocessing (Now uses updated `selected_num_features`)
    df, preprocessor = preprocess_data(df, selected_num_features, cat_features)

    # Train-Test Split
    X = df[selected_num_features + cat_features]
    y = df[target]
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=RANDOM_SEED)

    # Apply preprocessing to training data
    X_train_transformed = preprocessor.fit_transform(X_train)
    X_test_transformed = preprocessor.transform(X_test)

    # Feature Selection (after transformation)
    important_indices = select_important_features(X_train_transformed, y_train)

    # Select only important features
    X_train_selected = X_train_transformed[:, important_indices]
    X_test_selected = X_test_transformed[:, important_indices]

    # Train Model
    best_model = train_model(X_train_selected, y_train)

    # Evaluate Model
    evaluate_model(best_model, X_test_selected, y_test)

    # Save Model
    save_model(best_model)
